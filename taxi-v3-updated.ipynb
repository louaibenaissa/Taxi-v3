{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14de75ad",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Training an Agent to Play Taxi-v3\n",
    "**Author: Louai Abdallah BENAISSA**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-31T16:41:49.800994Z",
     "iopub.status.busy": "2025-01-31T16:41:49.800615Z",
     "iopub.status.idle": "2025-01-31T16:41:51.061497Z",
     "shell.execute_reply": "2025-01-31T16:41:51.059809Z",
     "shell.execute_reply.started": "2025-01-31T16:41:49.800953Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa1d64f",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards based on its actions and aims to maximize cumulative rewards over time.\n",
    "\n",
    "### The Reinforcement Learning Framework\n",
    "\n",
    "Reinforcement Learning consists of the following key components:\n",
    "- **Agent**: The learner or decision-maker.\n",
    "- **Environment**: The world in which the agent operates.\n",
    "- **State (s)**: A representation of the environment at a given time.\n",
    "- **Action (a)**: A choice the agent makes at a given state.\n",
    "- **Reward (r)**: A scalar value that the agent receives after taking an action.\n",
    "- **Policy (Ï€)**: A strategy that maps states to actions.\n",
    "- **Value Function (V)**: A function that estimates the expected cumulative reward of being in a state.\n",
    "- **Q-value (Q)**: A function that estimates the expected cumulative reward of taking an action in a state.\n",
    "\n",
    "![RL Framework](RL.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Taxi-v3 Environment\n",
    "\n",
    "The Taxi-v3 environment is a grid-based simulation where an agent (a taxi) must pick up and drop off passengers at designated locations while minimizing time and maximizing efficiency.\n",
    "\n",
    "### State Space\n",
    "The environment consists of a **5x5 grid**, with four designated locations. A state is represented by:\n",
    "- The taxi's position.\n",
    "- The passenger's location.\n",
    "- The passenger's destination.\n",
    "\n",
    "With **500 possible states**, the environment is small enough for tabular Q-learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T19:17:15.768785Z",
     "iopub.status.busy": "2025-01-30T19:17:15.768393Z",
     "iopub.status.idle": "2025-01-30T19:17:15.783030Z",
     "shell.execute_reply": "2025-01-30T19:17:15.781493Z",
     "shell.execute_reply.started": "2025-01-30T19:17:15.768755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space : 500\n",
      "Action space: 6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "state = env.reset()\n",
    "n_state = env.observation_space.n\n",
    "n_action = env.action_space.n\n",
    "print(f\"State space : {n_state}\")\n",
    "print(f\"Action space: {n_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning: A Model-Free Approach\n",
    "\n",
    "Q-learning is a model-free RL algorithm that estimates the **Q-value** for each state-action pair using the **Bellman equation**:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\gamma$ is the discount factor.\n",
    "- $r$ is the immediate reward.\n",
    "- $s'$ is the next state.\n",
    "- $\\max_{a'} Q(s', a')$ is the highest Q-value for the next state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T19:17:17.281426Z",
     "iopub.status.busy": "2025-01-30T19:17:17.281074Z",
     "iopub.status.idle": "2025-01-30T19:17:17.286499Z",
     "shell.execute_reply": "2025-01-30T19:17:17.285293Z",
     "shell.execute_reply.started": "2025-01-30T19:17:17.281398Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "# Initial Probability\n",
    "epsilon = 1\n",
    "# Rate of decay for epsilon\n",
    "prob_decay = 0.995\n",
    "# Minimum probability for the random actions\n",
    "min_epsilon = 0.001\n",
    "# Creating the Q-table for Q-learning without using a model\n",
    "Q_table = np.zeros((n_state,n_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T19:17:18.926545Z",
     "iopub.status.busy": "2025-01-30T19:17:18.926156Z",
     "iopub.status.idle": "2025-01-30T19:18:01.198191Z",
     "shell.execute_reply": "2025-01-30T19:18:01.196634Z",
     "shell.execute_reply.started": "2025-01-30T19:17:18.926512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Total reward : -1533\n",
      "Epoch : 10000, Total reward : 9\n",
      "Epoch : 20000, Total reward : 6\n",
      "Epoch : 30000, Total reward : 9\n",
      "Epoch : 40000, Total reward : 7\n",
      "Epoch : 50000, Total reward : 9\n",
      "Epoch : 60000, Total reward : 11\n",
      "Epoch : 70000, Total reward : 4\n",
      "Epoch : 80000, Total reward : 3\n",
      "Epoch : 90000, Total reward : 7\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "rewards = []\n",
    "for epoch in range(epochs) : \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0 \n",
    "    i = 0\n",
    "    while not done : \n",
    "        if random.uniform(0,1) < epsilon : \n",
    "            action = env.action_space.sample()\n",
    "        else : \n",
    "            action = np.argmax(Q_table[state])\n",
    "            \n",
    "        new_state,reward,done,truncated,info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        maxQ = np.max(Q_table[new_state])\n",
    "        stateQ = Q_table[state,action]\n",
    "        \n",
    "        Q_table[state,action] = stateQ*(1-alpha) + alpha*(reward + gamma*maxQ)\n",
    "        state = new_state\n",
    "        i+=1\n",
    "\n",
    "        epsilon = max(min_epsilon,epsilon*prob_decay)\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    if epoch %1000 == 0 : \n",
    "        print(f\"Epoch : {epoch}, Total reward : {total_reward}\")\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model \n",
    "We can use the policy learnt by the Q-table, to simulate the gameplay and save it in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T23:59:40.198657Z",
     "iopub.status.busy": "2025-01-29T23:59:40.198261Z",
     "iopub.status.idle": "2025-01-29T23:59:40.436339Z",
     "shell.execute_reply": "2025-01-29T23:59:40.435151Z",
     "shell.execute_reply.started": "2025-01-29T23:59:40.198630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode is 1 and total reward is : 11\n",
      "episode is 2 and total reward is : 7\n",
      "episode is 3 and total reward is : 4\n",
      "episode is 4 and total reward is : 5\n",
      "episode is 5 and total reward is : 10\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")  # Enable rendering\n",
    "env = RecordVideo(env, \"video\", episode_trigger=lambda episode_id: episode_id == 0,disable_logger=True)  \n",
    "\n",
    "# Evaluate the agent and record the video\n",
    "for i in range(5):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = np.argmax(Q_table[state])  # Choose the best action from the Q-table\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    print(f\"episode is {i+1} and total reward is : {total_reward}\")\n",
    "\n",
    "env.close()  # Close the environment and save the video"
   ]
  },

 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
